# -*- coding: utf-8 -*-
"""Recsys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1msYCU49XpcMIhxgKXbym5jNxE1blCgdt
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install lightfm

import pandas as pd
import numpy as np
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.evaluation import precision_at_k
import pyarrow.parquet as pq
from itertools import chain
import random

"""**Обучение на разделенном train 80/20**  
_____
"""

train_data = pd.read_csv('/content/drive/MyDrive/train_data.csv')

test_data = pd.read_csv('/content/drive/MyDrive/test_data.csv')

train_data.head()

test_data.head()

pd.concat([train_data['vacancy_id_'], test_data['vacancy_id_']]).unique()

# Подготовка данных для LightFM
dataset = Dataset()
dataset.fit(pd.concat([train_data['cookie_id'], test_data['cookie_id']]).unique(),\
            pd.concat([train_data['vacancy_id_'], test_data['vacancy_id_']]).unique())
num_users, num_items = dataset.interactions_shape()
num_users, num_items

# Создание словаря вакансий из dataset
dict_vacancy = {v: k for k, v in dataset.mapping()[2].items()}

# Создание матрицы взаимодействий
train_interactions, train_weights = dataset.build_interactions([(x['cookie_id'], x['vacancy_id_'], x['event_type']) for idx, x in train_data.iterrows()])
test_interactions, test_weights = dataset.build_interactions([(x['cookie_id'], x['vacancy_id_'], x['event_type']) for idx, x in test_data.iterrows()])

train_interactions

test_interactions

# Обучение модели LightFM
model = LightFM(loss='warp')
model.fit(train_interactions, epochs=15)

# Оценка модели с использованием метрики precision@5 на test
# check_intesection=True (train_interactions are supplied)
test_precision = precision_at_k(model, test_interactions, train_interactions = train_interactions, k=5).mean()
print('Test precision@5:', test_precision)
# w/o features: Test precision@5: 0.058370296

"""**Добавление признаков в item**  
___
"""

item_features_df = pd.read_csv('/content/drive/MyDrive/item_feature_conv.csv')

item_features_df_decay = pd.read_csv('/content/drive/MyDrive/item_feature_decay.csv')

item_features_df

pd.cut(item_features_df_decay['time_decay'], 3).value_counts()

item_features_df_decay['cat_time_decay'] = pd.cut(item_features_df_decay['time_decay'], 3, labels = ['d1', 'd2', 'd3'])

item_features_df_decay

item_feature_data = pd.merge(item_features_df, item_features_df_decay, on = 'vacancy_id_')
item_feature_data

# Подготовка данных для LightFM via item_feature
dataset_item = Dataset()
dataset_item.fit(pd.concat([train_data['cookie_id'], test_data['cookie_id']]).unique(),\
            pd.concat([train_data['vacancy_id_'], test_data['vacancy_id_']]).unique(),\
            #item_features=['c1', 'c2', 'c3', 'c4', 'c5'])
            item_features=['c1', 'c2', 'c3', 'c4', 'c5', 'd1', 'd2', 'd3'])

# Создание словаря вакансий из dataset_item
dict_vac_item = {v: k for k, v in dataset_item.mapping()[2].items()}

train_interactions_item, train_weights_item = dataset_item.build_interactions([(x['cookie_id'], x['vacancy_id_'], x['event_type']) for idx, x in train_data.iterrows()])

train_interactions_item

test_interactions_item, test_weights_item = dataset_item.build_interactions([(x['cookie_id'], x['vacancy_id_'], x['event_type']) for idx, x in test_data.iterrows()])

# Создание матрицы item_feature
#item_features = dataset_item.build_item_features([(x['vacancy_id_'], [x['cat_conv']]) for idx, x in item_features_df.iterrows()])
item_features = dataset_item.build_item_features([(x['vacancy_id_'], [x['cat_conv'], x['cat_time_decay']]) for idx, x in item_feature_data.iterrows()])

item_features

# Создаем модель
model_item = LightFM(loss='warp')

# Обучаем модель на train данных via item_feature
model_item.fit(train_interactions_item, item_features=item_features, epochs=15)

# Оценка модели с использованием метрики precision@5 на test
# check_intesection=True (train_interactions are supplied)
test_precision_item = precision_at_k(model_item, test_interactions_item, train_interactions = train_interactions_item, item_features=item_features, k=5).mean()
print('Test precision@5:', test_precision_item)
# item_features(conversion category) Test precision@5: 0.055422664
# item_features(time decay category) Test precision@5: 0.044285066
# item_features(conversion category + time decay category) Test precision@5: 0.03927076

"""___  
**Обучение на полном train**
"""

data_full = pd.concat([train_data, test_data])
data_full

# Создание матрицы взаимодействий
train_interactions_full, train_weights_full = dataset.build_interactions([(x['cookie_id'], x['vacancy_id_'], x['event_type']) for idx, x in data_full.iterrows()])

train_interactions_full

# Обучение модели LightFM на полном train w/o features
model_full = LightFM(loss='warp')
model_full.fit(train_interactions_full, epochs=15)

"""**Валидация на test_public**  
___
"""

# Чтение файла
table_test = pq.read_table('/content/drive/MyDrive/test_public_mfti.parquet')

# Запись в датафрейм
df_test_public = table_test.to_pandas()
df_test_public

# Функция для вычисления Precision@k для LightFM
def evaluate_precision_at_k(model_ev, test_data_df, dataset_ev, dict_ev, k=5):
    test_users = test_data_df['cookie_id'].unique()
    precision = 0
    
    for user_cookie_id in test_users:
        known_positives = data_full[data_full['cookie_id'] == user_cookie_id]['vacancy_id_'].unique()
        true_positives = chain.from_iterable(test_data_df[test_data_df['cookie_id'] == user_cookie_id]['vacancy_id_'])

        user_idx = dataset_ev.mapping()[0][user_cookie_id] # Доступ к словарю user-item
        scores = model_ev.predict(user_idx, np.arange(dataset_ev.item_features_shape()[0]))
        top_items = np.argsort(-scores)
        top_items_ = [dict_ev[idx] for idx in top_items]
        
        # Фильтруем уже просмотренные вакансии
        new_top_items = [x for x in top_items_ if x not in known_positives][:k]

        precision += len(set(new_top_items) & set(true_positives)) / k

    return precision / len(test_users)

lightfm_precision = evaluate_precision_at_k(model, df_test_public, dataset, dict_vacancy, k=5)
print(f"LightFM Precision@5: {lightfm_precision}")
# w/o features train 80/20 - LightFM Precision@5: 0.036010362694300455

lightfm_precision_full = evaluate_precision_at_k(model_full, df_test_public, dataset, dict_vacancy, k=5)
print(f"LightFM Precision@5: {lightfm_precision_full}")
# w/o features full train - LightFM Precision@5: 0.03419689119170978

lightfm_precision_item = evaluate_precision_at_k(model_item, df_test_public, dataset_item, dict_vac_item, k=5)
print(f"LightFM Precision@5: {lightfm_precision_item}")
# item_features(conversion category) train 80/20 - LightFM Precision@5: 0.03911917098445587
# item_features(time_decay category) train 80/20 - LightFM Precision@5: 0.009067357512953372
# item_features(conversion category + time_decay category) train 80/20 - LightFM Precision@5: 0.008549222797927466

"""**Рекомендации**  
___
"""

# df_test_private
# Чтение файла
table_test_pr = pq.read_table('/content/drive/MyDrive/test_private_users_mfti.parquet')

# Запись в датафрейм
df_test_private = table_test_pr.to_pandas()

def recommend_user(model_rec, test_data_proba, dataset_rec, dict_vacancy_rec, k=5):
    test_users = test_data_proba['cookie_id'].unique()
    df = pd.Dataframe(columns=['cookie_id', 'vacancy_id_'])
   
    for user_cookie_id in test_users:
        known_positives = data_full[data_full['cookie_id'] == user_cookie_id]['vacancy_id_'].unique()

        user_idx = dataset_rec.mapping()[0][user_cookie_id] # Доступ к словарю user-item
        scores = model_rec.predict(user_idx, np.arange(dataset_rec.item_features_shape()[0]))
        top_items = np.argsort(-scores)
        top_items_ = [dict_vacancy_rec[idx] for idx in top_items]
        
        # Фильтруем уже просмотренные вакансии
        new_top_items = [x for x in top_items_ if x not in known_positives][:k]
        
        df.loc[len(df)] = [user_cookie_id, new_top_items]
        
    return df

rec_df = recommend_user(model, df_test_private, dataset, dict_vacancy, k=5)

rec_df.to_csv('test_private_rec.csv', index=False)

"""**Baseline**  
____________________
"""

# Топ-100 вакансий из train (80%)
top_100_vacancies_80 = train_data.groupby(['vacancy_id_']).agg({'event_type': 'sum'}).reset_index()\
.sort_values(by='event_type', ascending=False).head(100)['vacancy_id_'].values

top_100_vacancies_80

# Топ-100 вакансий из всего train
top_100_vacancies = data_full.groupby(['vacancy_id_']).agg({'event_type': 'sum'}).reset_index()\
.sort_values(by='event_type', ascending=False).head(100)['vacancy_id_'].values

top_100_vacancies

# baseline модель, которая возвращает топ-k вакансий для всех пользователей
def baseline_model(known_positives, top_100, k=5):
    top_items = random.sample([x for x in top_100 if x not in known_positives], k=k)
    return top_items

# Вычисление Precision@k для baseline модели
def evaluate_precision_at_k_baseline(data_df, test_df, top):
  baseline_precision = 0

  test_users = test_df['cookie_id'].unique()
  for user_cookie_id in test_users:
      known_positives = data_df[data_df['cookie_id'] == user_cookie_id]['vacancy_id_'].unique()
      true_positives = chain.from_iterable(test_df[test_df['cookie_id'] == user_cookie_id]['vacancy_id_'])

      top_items = baseline_model(known_positives, top, k=5)
      baseline_precision += len(set(top_items) & set(true_positives)) / 5

  baseline_precision /= len(test_users)
  print(f'Baseline Precision@5: {baseline_precision}')

evaluate_precision_at_k_baseline(train_data, test_data, top_100_vacancies_80)
# Local validate - Baseline Precision@5 для test - train (20%): 0.00955391567513852

evaluate_precision_at_k_baseline(data_full, df_test_public, top_100_vacancies)
# Baseline Precision@5 для test public: 0.011139896373056997

"""_______________"""